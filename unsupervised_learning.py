# -*- coding: utf-8 -*-
"""Unsupervised learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A4wfASBTHelnRmKpIVKkopbwgCQN5Ilv

#Unsupervised learning (Kohonen network)
"""

import numpy as np
import pandas as pd

from google.colab import files
uploaded = files.upload()
for fn in uploaded.keys():
  print('The user has loaded the data file "{name}" with a length of {length} bits'.format(
      name=fn, length=len(uploaded[fn])))

"""#Attribute Information

* age
* sex
* chest pain type (4 values)
* resting blood pressure
* serum cholestoral in mg/dl
* fasting blood sugar > 120 mg/dl
* resting electrocardiographic results (values 0,1,2)
* maximum heart rate achieved
* exercise induced angina
* oldpeak = ST depression induced by exercise relative to rest
* the slope of the peak exercise ST segment
* number of major vessels (0-3) colored by flourosopy
* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect
"""

#Loading data
dataset = pd.read_csv('heart.csv')

import matplotlib.pyplot as plt

#Columns and rows in the data
print(dataset.shape)
#Top 10 rows
print(dataset.head(10))
#Main data matrices
print(dataset.describe())
#Class distribution
from pylab import rcParams
rcParams['figure.figsize'] = 10, 7
print(dataset.groupby('target').size())
dataset['target'].value_counts().plot.bar()

print('The proportion of classes:')
print(dataset['target'].value_counts() / len(dataset))

#Data types
print(dataset.info())

#Number of missing data points per column
missing_values_count = dataset.isnull().sum()
print(missing_values_count)

#Histograms
dataset.hist(figsize=(15,12),bins = 20)
plt.title("Distribution features")
plt.show()

"""#Pre-processing of data"""

X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

print (X.shape, y.shape)

##The function scales the data
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X = sc_X.fit_transform(X)

#The second of the data scaling algorithms
#from sklearn.preprocessing import MinMaxScaler
#sc_X = MinMaxScaler(feature_range = (0, 1))
#X = sc_X.fit_transform(X)

"""#Construction of the Kohonen network"""

#Installation of MiniSom - Kohonen Network library
! pip install minisom

#Network size (x,y), input vector = 13, sigma (8,8) = 1.0; sigma (16,16) = 1.3;
from minisom import MiniSom
som = MiniSom(x = 16, y = 16, input_len = 13, sigma = 1.3, learning_rate = 0.5)
som.random_weights_init(X) #starting a network with random weights

print ("Kohonen network initialization")
som.train_random(data = X, num_iteration = 5000) #number of iterations during training
print ("...\nProcessing complete")

from pylab import axis, bone, pcolor, colorbar, plot, show
bone()
pcolor(som.distance_map().T)
colorbar()

y[y == 0] = 0
y[y == 1] = 1
markers = ['x', 'o']
colors = ['r', 'g']
for i, xx in enumerate(X):
    w = som.winner(xx)
    plot(w[0] + 0.5, w[1] + 0.5, 
         markers[y[i]],
         markeredgecolor = colors[y[i]],
         markerfacecolor = 'None',
         markersize = 12,
         markeredgewidth = 2)
axis([0, som.get_weights().shape[0],0,som.get_weights().shape[1]])
show()

"""#Definition of classification"""

#Assigning map to classes
class_assignments = som.labels_map(X, y)

#Defined classification function based on Kohonen's map
def classify(som, data, class_assignments):
    """Classifies each sample in data in one of the classes definited
    using the method labels_map.
    Returns a list of the same length of data where the i-th element
    is the class assigned to data[i]. """
    winmap = class_assignments
    default_class = np.sum(list(winmap.values())).most_common()[0][0]
    result = []
    for d in data:
        win_position = som.winner(d)
        if win_position in winmap:
            result.append(winmap[win_position].most_common()[0][0])
        else:
            result.append(default_class)
    return result

#Division of the set
from sklearn import model_selection
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=0)
print (X_train.shape, X_test.shape, y_train.shape, y_test.shape)

"""#Analysis of the results"""

#Predict test suite results
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
class_assignments = som.labels_map(X_train, y_train)
print(classification_report(y_test, classify(som, X_test, class_assignments)))

#y_pred = model.predict(X_test)
#y_pred = (y_pred>0.5) #Because the output is a probability
cm = confusion_matrix(y_test, classify(som, X_test, class_assignments))
accuracy = accuracy_score(y_test, classify(som, X_test, class_assignments))
print("Classification (Kohonen network):")
print("Accuracy = ", accuracy)
print(cm)